{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["# üìò Sesi√≥n 11: PyTorch - Deep Learning\n\n---\n\n## üéØ Objetivos\n\n- Dominar tensores y autograd\n- Construir redes neuronales\n- Entrenar modelos con optimizaci√≥n\n- Usar Datasets y DataLoaders"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA disponible: {torch.cuda.is_available()}\")"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 1. Tensores"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Crear tensores\na = torch.tensor([1, 2, 3, 4])\nb = torch.zeros(2, 3)\nc = torch.ones(2, 3)\nd = torch.randn(2, 3)  # Normal(0, 1)\n\nprint(f\"Tensor a: {a}\")\nprint(f\"Shape: {a.shape}, Dtype: {a.dtype}\")\nprint(f\"\\nRandom tensor:\\n{d}\")"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Operaciones\nx = torch.tensor([1.0, 2.0, 3.0])\ny = torch.tensor([4.0, 5.0, 6.0])\n\nprint(f\"Suma: {x + y}\")\nprint(f\"Producto: {x * y}\")\nprint(f\"Dot product: {torch.dot(x, y)}\")\nprint(f\"Matmul: {torch.mm(x.view(3, 1), y.view(1, 3))}\")\n\n# Reshape\nz = torch.arange(12).reshape(3, 4)\nprint(f\"\\nReshaped:\\n{z}\")"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Conversi√≥n NumPy <-> PyTorch\nnp_array = np.array([1, 2, 3])\ntensor = torch.from_numpy(np_array)\nback_to_numpy = tensor.numpy()\n\nprint(f\"NumPy: {np_array}\")\nprint(f\"Tensor: {tensor}\")\nprint(f\"Back to NumPy: {back_to_numpy}\")"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 2. Autograd"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Gradientes autom√°ticos\nx = torch.tensor([2.0, 3.0], requires_grad=True)\n\n# Forward pass\ny = x ** 2\nz = y.sum()\n\n# Backward pass\nz.backward()\n\nprint(f\"x: {x}\")\nprint(f\"y = x¬≤: {y}\")\nprint(f\"z = sum(y): {z}\")\nprint(f\"dz/dx: {x.grad}\")  # Deber√≠a ser 2*x = [4, 6]"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Ejemplo: descenso de gradiente manual\nx = torch.tensor([5.0], requires_grad=True)\nlr = 0.1\n\nhistorial = []\nfor i in range(20):\n    # f(x) = (x - 2)¬≤\n    y = (x - 2) ** 2\n    historial.append((x.item(), y.item()))\n    \n    y.backward()\n    \n    with torch.no_grad():\n        x -= lr * x.grad\n    \n    x.grad.zero_()\n\nprint(f\"√ìptimo encontrado: x = {x.item():.4f}\")\nplt.plot([h[0] for h in historial], [h[1] for h in historial], 'o-')\nplt.xlabel('x'); plt.ylabel('f(x)'); plt.title('Convergencia')\nplt.show()"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 3. Redes Neuronales con nn.Module"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Definir red neuronal\nclass RedNeuronal(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super().__init__()\n        self.layer1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.layer2 = nn.Linear(hidden_size, output_size)\n    \n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.relu(x)\n        x = self.layer2(x)\n        return x\n\nmodel = RedNeuronal(10, 20, 2)\nprint(model)\nprint(f\"\\nPar√°metros: {sum(p.numel() for p in model.parameters())}\")"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Red con nn.Sequential\nmodel_seq = nn.Sequential(\n    nn.Linear(10, 64),\n    nn.ReLU(),\n    nn.Dropout(0.2),\n    nn.Linear(64, 32),\n    nn.ReLU(),\n    nn.Linear(32, 2)\n)\n\nprint(model_seq)"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 4. Entrenamiento Completo"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Generar datos sint√©ticos\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n\n# Convertir a tensores\nX_tensor = torch.FloatTensor(X)\ny_tensor = torch.LongTensor(y)\n\n# Split\ntrain_size = int(0.8 * len(X_tensor))\nX_train, X_test = X_tensor[:train_size], X_tensor[train_size:]\ny_train, y_test = y_tensor[:train_size], y_tensor[train_size:]\n\nprint(f\"Train: {X_train.shape}, Test: {X_test.shape}\")"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# DataLoader\ntrain_dataset = TensorDataset(X_train, y_train)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n\ntest_dataset = TensorDataset(X_test, y_test)\ntest_loader = DataLoader(test_dataset, batch_size=32)"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Modelo, loss, optimizer\nmodel = nn.Sequential(\n    nn.Linear(10, 64),\n    nn.ReLU(),\n    nn.Linear(64, 32),\n    nn.ReLU(),\n    nn.Linear(32, 2)\n)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Training loop\nnum_epochs = 50\nhistory = {'train_loss': [], 'test_acc': []}\n\nfor epoch in range(num_epochs):\n    model.train()\n    epoch_loss = 0\n    \n    for batch_X, batch_y in train_loader:\n        # Forward\n        outputs = model(batch_X)\n        loss = criterion(outputs, batch_y)\n        \n        # Backward\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n    \n    # Evaluaci√≥n\n    model.eval()\n    with torch.no_grad():\n        correct = 0\n        total = 0\n        for batch_X, batch_y in test_loader:\n            outputs = model(batch_X)\n            _, predicted = torch.max(outputs.data, 1)\n            total += batch_y.size(0)\n            correct += (predicted == batch_y).sum().item()\n        \n        accuracy = correct / total\n    \n    history['train_loss'].append(epoch_loss / len(train_loader))\n    history['test_acc'].append(accuracy)\n    \n    if (epoch + 1) % 10 == 0:\n        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_loader):.4f}, Acc: {accuracy:.4f}\")"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Visualizar entrenamiento\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\naxes[0].plot(history['train_loss'])\naxes[0].set_title('Training Loss')\naxes[0].set_xlabel('Epoch')\n\naxes[1].plot(history['test_acc'])\naxes[1].set_title('Test Accuracy')\naxes[1].set_xlabel('Epoch')\n\nplt.tight_layout()\nplt.show()"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 5. GPU (si disponible)"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Usar GPU si est√° disponible\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Usando: {device}\")\n\n# Mover modelo a GPU\n# model = model.to(device)\n# X_train = X_train.to(device)\n# y_train = y_train.to(device)"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["---\n## üèãÔ∏è Ejercicios Resueltos"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Ejercicio 1: Regresi√≥n con PyTorch\nX_reg = torch.linspace(-5, 5, 100).reshape(-1, 1)\ny_reg = 3 * X_reg + 2 + torch.randn_like(X_reg) * 0.5\n\nmodel_reg = nn.Linear(1, 1)\noptimizer = optim.SGD(model_reg.parameters(), lr=0.01)\ncriterion = nn.MSELoss()\n\nfor epoch in range(100):\n    pred = model_reg(X_reg)\n    loss = criterion(pred, y_reg)\n    \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\nprint(f\"Peso aprendido: {model_reg.weight.item():.2f} (real: 3)\")\nprint(f\"Bias aprendido: {model_reg.bias.item():.2f} (real: 2)\")"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["---\n## üìù Ejercicios para Practicar"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Ejercicio 1: Implementar red con Batch Normalization\n# Tu c√≥digo aqu√≠"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Ejercicio 2: Early stopping y guardar mejor modelo\n# Tu c√≥digo aqu√≠"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Ejercicio 3: Dataset personalizado para im√°genes\n# Tu c√≥digo aqu√≠"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["---\n## üéØ Resumen\n\n- **Tensores**: torch.tensor(), operaciones, device\n- **Autograd**: requires_grad, backward(), grad\n- **nn.Module**: Definir arquitectura, forward()\n- **Entrenamiento**: DataLoader, optimizer, criterion, training loop\n- **GPU**: .to(device), cuda/cpu"]
  }
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}},
 "nbformat": 4,
 "nbformat_minor": 4
}
